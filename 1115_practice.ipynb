{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFh3dlP3NcP4HNI6QxvQE2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/24_fall_textmining_NLP/blob/main/1115_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 설치가 필요하다면 먼저 TensorFlow 및 Keras를 설치하세요.\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np\n",
        "\n",
        "# 예시 문장 정의 (임베딩을 생성할 문장들)\n",
        "texts = [\n",
        "    \"TensorFlow is a popular deep learning framework\",\n",
        "    \"Keras makes building neural networks easy\",\n",
        "    \"Embedding layers are useful for NLP tasks\"\n",
        "]\n",
        "\n",
        "# 토크나이저를 사용하여 문장을 정수 인덱스로 변환\n",
        "# 최대 10,000개의 단어를 사용하는 토크나이저 객체 생성\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "# 텍스트를 학습하여 어휘 사전을 구축하고, 각 문장을 정수 시퀀스로 변환\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sample_sentences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 패딩을 위한 pad_sequences 사용\n",
        "# 각 문장의 길이를 10으로 맞추기 위해 'post' 방식으로 패딩 (문장의 끝에 0을 추가)\n",
        "sample_sentences = pad_sequences(sample_sentences,\n",
        "                                 maxlen=10,\n",
        "                                 padding='post')\n",
        "\n",
        "# 예시 토큰 수 및 임베딩 차원 정의\n",
        "vocab_size = 10000  # 어휘 크기 (단어의 개수)\n",
        "embedding_dim = 16  # 임베딩 벡터의 차원 (각 단어를 16차원 벡터로 변환)\n",
        "\n",
        "# Sequential 모델을 사용하여 Embedding Layer 추가\n",
        "model = Sequential()\n",
        "# Embedding 레이어 추가: 각 입력 단어를 임베딩 벡터로 변환\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    input_length=10, name='embedding_layer'))\n",
        "\n",
        "# 모델 컴파일\n",
        "# 옵티마이저는 'adam'을 사용하고, 손실 함수는 'mse'(평균 제곱 오차) 사용\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 입력 데이터를 넘겨 임베딩 벡터 생성\n",
        "# 학습 없이 각 문장의 임베딩 벡터를 생성 (임베딩 레이어만 사용)\n",
        "embeddings = model.predict(np.array(sample_sentences))\n",
        "\n",
        "# 결과 출력\n",
        "# 임베딩 벡터 출력 (각 문장의 단어들이 16차원 벡터로 변환된 결과)\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    print(f\"Sentence {i+1} embedding shape: {embedding.shape}\")\n",
        "    print(f\"Sentence {i+1} embedding: {embedding}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9mkrpjGGw9r",
        "outputId": "09d364c6-c6eb-45b5-8621-bbc6371b89d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "Sentence 1 embedding shape: (10, 16)\n",
            "Sentence 1 embedding: [[ 0.04252852  0.02998107  0.01326009 -0.00825164 -0.00788052 -0.01260535\n",
            "  -0.01270292 -0.01431248 -0.03161935  0.00586275 -0.00892152  0.0418948\n",
            "  -0.00966151  0.00959768 -0.04670391  0.00136875]\n",
            " [-0.02636283 -0.04943597  0.00539342 -0.0021412  -0.04119717 -0.04665187\n",
            "  -0.02471846 -0.04501973 -0.01486772  0.01970757  0.01020201 -0.00817887\n",
            "  -0.03032863 -0.03572693 -0.0031454  -0.04489181]\n",
            " [ 0.02105604  0.04415658  0.02235423 -0.03303702  0.00814684 -0.01458628\n",
            "  -0.03003329  0.04907861 -0.01559714  0.03055868  0.03272815 -0.00089258\n",
            "   0.04369364 -0.04630911  0.02511437 -0.02555539]\n",
            " [ 0.02369679 -0.01959225  0.04836667  0.04902769  0.0271662   0.02518553\n",
            "   0.00834912 -0.03897066 -0.02579061  0.01553797 -0.00302776  0.02273175\n",
            "   0.00304837  0.0015753  -0.02277663  0.01319453]\n",
            " [-0.04614007  0.0064952   0.02878653  0.00668649  0.0235536   0.02773559\n",
            "  -0.01038239  0.02923566 -0.03441047  0.02311856  0.01408895  0.01260065\n",
            "  -0.04726459 -0.0038094   0.03553298 -0.03690109]\n",
            " [ 0.03083352 -0.02512784  0.00522406  0.01995886  0.00034201 -0.01928431\n",
            "  -0.04769722 -0.02123407  0.03338036 -0.02866417 -0.03633219 -0.04590408\n",
            "   0.04967824  0.04613062  0.04195103 -0.00741553]\n",
            " [-0.04011989  0.01932284 -0.03646296 -0.01567189  0.0360758   0.00436622\n",
            "  -0.02070044 -0.03224545 -0.04892803  0.00727141 -0.00180361 -0.02884059\n",
            "   0.00580011  0.02837244  0.0110416  -0.0212867 ]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]]\n",
            "Sentence 2 embedding shape: (10, 16)\n",
            "Sentence 2 embedding: [[-0.00880494  0.00176544 -0.03985112 -0.02703339  0.04916378  0.03045148\n",
            "   0.01004249 -0.02534091  0.02544408 -0.02280277 -0.02820875  0.04090761\n",
            "   0.04674919 -0.03729898 -0.03272414  0.0176683 ]\n",
            " [-0.02628964 -0.04233949  0.02802733 -0.03589169 -0.02879517  0.04800096\n",
            "  -0.01759196  0.00547006  0.02801586 -0.00334914 -0.0158698   0.01018782\n",
            "  -0.03668071 -0.01848701 -0.01132103  0.04467701]\n",
            " [-0.01634578  0.01105805  0.02887725  0.00810047  0.03220627 -0.01718984\n",
            "  -0.03376086 -0.04140576  0.00690893 -0.03914719 -0.01886064 -0.00547475\n",
            "   0.04821402 -0.03258981  0.01384002  0.03045646]\n",
            " [-0.04140512  0.04278549 -0.02999134 -0.04706487 -0.01183446  0.02203672\n",
            "   0.04998023 -0.02968512 -0.00837976  0.00677709  0.0265384  -0.02557352\n",
            "  -0.03711195 -0.01752571  0.01175248  0.03525165]\n",
            " [-0.02969235 -0.04443013  0.03518763 -0.04009267 -0.01280262  0.00857554\n",
            "   0.02356769  0.02795191  0.00794518  0.04519952  0.00585786 -0.02874656\n",
            "   0.04137165 -0.03557477 -0.03558294 -0.02214092]\n",
            " [ 0.00809113 -0.00408802  0.04569078 -0.01628649  0.01338449  0.01777006\n",
            "   0.01487198  0.02839685 -0.0339575  -0.00687829 -0.0012604   0.00397766\n",
            "  -0.04398364  0.02817247 -0.04443029 -0.02657081]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]]\n",
            "Sentence 3 embedding shape: (10, 16)\n",
            "Sentence 3 embedding: [[-0.04186947  0.04837003 -0.0197809  -0.02536727 -0.04688699  0.02240929\n",
            "   0.0114358  -0.04531208 -0.01169281 -0.00392992 -0.01860328  0.03180892\n",
            "   0.01825677  0.02741777 -0.04321409  0.00468938]\n",
            " [ 0.04504535 -0.0035138  -0.00082759  0.01328346 -0.00853161  0.02612224\n",
            "  -0.01986207  0.03758203 -0.04875819  0.02862804  0.04586998  0.03089874\n",
            "   0.0128971  -0.02208759  0.02469936  0.02753223]\n",
            " [ 0.00562972 -0.01854868  0.02507858  0.00980579 -0.04229509  0.00885657\n",
            "  -0.02742988 -0.0342777   0.04140702 -0.04114569  0.04564302  0.04222203\n",
            "   0.01644211 -0.0329931   0.00263158  0.01192721]\n",
            " [-0.02800044 -0.01825501 -0.03694271  0.02847819 -0.02825484  0.04766371\n",
            "   0.04067923 -0.00839899  0.03361071  0.00550967  0.00641399  0.04644467\n",
            "  -0.04961112  0.0131484  -0.01867685  0.03485182]\n",
            " [-0.03427627 -0.03602028 -0.00067096  0.01100599  0.00804696 -0.0089465\n",
            "   0.02002862 -0.01208788 -0.02417698 -0.01547157  0.04407403 -0.03198053\n",
            "   0.01152629 -0.04642571 -0.00062339 -0.04094397]\n",
            " [ 0.0102294  -0.03646256 -0.01927807 -0.01479572 -0.00889973 -0.02199327\n",
            "  -0.04760356 -0.04943125  0.02192329  0.01248413  0.0295851  -0.02896713\n",
            "   0.01697805  0.01122753  0.04608587 -0.04676891]\n",
            " [ 0.04133972  0.02281429  0.04573831  0.00831263  0.03183745 -0.0355243\n",
            "  -0.02985578 -0.02302542 -0.04756609 -0.03822865  0.00253729  0.02169741\n",
            "  -0.03056574 -0.01234118 -0.04155873 -0.03528122]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]\n",
            " [ 0.00777791  0.03486495 -0.00763189  0.00226535 -0.04885764  0.04846836\n",
            "  -0.00981842 -0.03591524 -0.01589583 -0.04641715  0.03350661 -0.04924778\n",
            "   0.04508902  0.03733047  0.02313391 -0.00839575]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 설치가 필요하다면 먼저 TensorFlow 및 Keras를 설치하세요.\n",
        "# !pip install tensorflow\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense\n",
        "import numpy as np\n",
        "\n",
        "# 예시 문장 정의 (임베딩을 생성할 문장들)\n",
        "texts = [\n",
        "    \"TensorFlow is a popular deep learning framework\",\n",
        "    \"Keras makes building neural networks easy\",\n",
        "    \"Embedding layers are useful for NLP tasks\"\n",
        "]\n",
        "\n",
        "# 토크나이저를 사용하여 문장을 정수 인덱스로 변환\n",
        "# 최대 10,000개의 단어를 사용하는 토크나이저 객체 생성\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "# 텍스트를 학습하여 어휘 사전을 구축하고, 각 문장을 정수 시퀀스로 변환\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sample_sentences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 패딩을 위한 pad_sequences 사용\n",
        "# 각 문장의 길이를 10으로 맞추기 위해 'post' 방식으로 패딩 (문장의 끝에 0을 추가)\n",
        "sample_sentences = pad_sequences(sample_sentences, maxlen=10, padding='post')\n",
        "\n",
        "# 예시 토큰 수 및 임베딩 차원 정의\n",
        "vocab_size = 10000  # 어휘 크기 (단어의 개수)\n",
        "embedding_dim = 16  # 임베딩 벡터의 차원 (각 단어를 16차원 벡터로 변환)\n",
        "\n",
        "# Sequential 모델을 사용하여 Embedding Layer 추가\n",
        "model = Sequential()\n",
        "# Embedding 레이어 추가: 각 입력 단어를 임베딩 벡터로 변환\n",
        "model.add(Embedding(input_dim=vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    input_length=10, name='embedding_layer'))\n",
        "\n",
        "# Dense 레이어 추가: 추가적인 학습을 위한 완전 연결 레이어\n",
        "model.add(Dense(8, activation='relu', name='dense_layer_1'))\n",
        "\n",
        "# 모델 컴파일\n",
        "# 옵티마이저는 'adam'을 사용하고, 손실 함수는 'mse'(평균 제곱 오차) 사용\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 입력 데이터를 넘겨 임베딩 벡터 생성\n",
        "# 학습 없이 각 문장의 임베딩 벡터를 생성 (임베딩 레이어만 사용)\n",
        "embeddings = model.predict(np.array(sample_sentences))\n",
        "\n",
        "# 결과 출력\n",
        "# 임베딩 벡터 출력 (각 문장의 단어들이 16차원 벡터로 변환된 결과)\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    print(f\"Sentence {i+1} embedding shape: {embedding.shape}\")\n",
        "    print(f\"Sentence {i+1} embedding: {embedding}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK7gfoF8IBhS",
        "outputId": "f9662fec-0baf-4296-da9f-f6cbe1bd9962"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
            "Sentence 1 embedding shape: (10, 8)\n",
            "Sentence 1 embedding: [[0.01009698 0.         0.         0.01263447 0.03688491 0.\n",
            "  0.05713772 0.        ]\n",
            " [0.00486918 0.         0.         0.00476349 0.01815758 0.\n",
            "  0.0563328  0.        ]\n",
            " [0.         0.05086231 0.         0.         0.         0.01587279\n",
            "  0.00381816 0.01862209]\n",
            " [0.05723408 0.         0.01656855 0.         0.         0.02404143\n",
            "  0.06831724 0.00884636]\n",
            " [0.         0.         0.02369934 0.00063804 0.0704447  0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.02407532 0.         0.         0.05570051\n",
            "  0.02693674 0.03290957]\n",
            " [0.         0.01475365 0.         0.03504165 0.         0.02191207\n",
            "  0.04428962 0.04760284]\n",
            " [0.00133397 0.02701646 0.00124334 0.00681645 0.05828975 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124333 0.00681645 0.05828976 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124333 0.00681645 0.05828976 0.\n",
            "  0.         0.        ]]\n",
            "Sentence 2 embedding shape: (10, 8)\n",
            "Sentence 2 embedding: [[0.02083284 0.00043655 0.         0.00969324 0.02561582 0.\n",
            "  0.         0.03756657]\n",
            " [0.         0.01766241 0.01372644 0.         0.         0.04359812\n",
            "  0.01287828 0.        ]\n",
            " [0.         0.02399497 0.         0.03956354 0.00812566 0.\n",
            "  0.         0.        ]\n",
            " [0.01198166 0.         0.03612018 0.         0.01465468 0.01950997\n",
            "  0.         0.01548974]\n",
            " [0.00939925 0.         0.         0.02649653 0.         0.05346207\n",
            "  0.03645404 0.08056433]\n",
            " [0.         0.0070203  0.01266353 0.01170916 0.00630738 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124334 0.00681645 0.05828975 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124334 0.00681645 0.05828975 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124333 0.00681645 0.05828976 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124333 0.00681645 0.05828976 0.\n",
            "  0.         0.        ]]\n",
            "Sentence 3 embedding shape: (10, 8)\n",
            "Sentence 3 embedding: [[0.01021816 0.         0.         0.         0.         0.03188512\n",
            "  0.         0.05682241]\n",
            " [0.         0.02564892 0.         0.         0.         0.01471875\n",
            "  0.         0.        ]\n",
            " [0.01018073 0.01675814 0.         0.02887686 0.00772468 0.\n",
            "  0.05366196 0.        ]\n",
            " [0.02099863 0.         0.01027511 0.00237564 0.00929393 0.01795781\n",
            "  0.04298908 0.        ]\n",
            " [0.         0.         0.         0.02225642 0.         0.\n",
            "  0.01828754 0.05478408]\n",
            " [0.         0.07998444 0.         0.01740129 0.         0.03023305\n",
            "  0.         0.        ]\n",
            " [0.02611682 0.03393561 0.         0.         0.         0.00793036\n",
            "  0.         0.00486495]\n",
            " [0.00133397 0.02701646 0.00124334 0.00681645 0.05828975 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124333 0.00681645 0.05828976 0.\n",
            "  0.         0.        ]\n",
            " [0.00133397 0.02701646 0.00124333 0.00681645 0.05828976 0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8YdCpO_IXLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}