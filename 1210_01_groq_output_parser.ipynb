{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw+zXy/hifm/mrC6WkOwQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/24_fall_textmining_NLP/blob/main/1210_01_groq_output_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Groq API](https://wikidocs.net/259655)**\n",
        "\n",
        "- https://console.groq.com/playground\n",
        "- https://python.langchain.com/docs/how_to/sequence/ **(Langchain Tutorial)**\n",
        "- https://wikidocs.net/book/14314 **(한글판 랑체인 튜토리얼)**"
      ],
      "metadata": {
        "id": "UmjMoIobUZr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_key = userdata.get('groq')"
      ],
      "metadata": {
        "id": "GuxZqRXkU-aA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-dOigfFVNyr",
        "outputId": "d37aa4ee-b06c-4797-b351-5c608634369f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0ptXYOaAUUAR"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# ChatGroq 모델 초기화\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\", # google/gemma-2-9b-it\n",
        "    temperature=0.7,\n",
        "    max_tokens=300,\n",
        "    api_key=groq_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYFHZ-QVMZJ1",
        "outputId": "80c57cb9-fb30-4f29-df59-a344380a1d74"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7c9bff765b10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c9bff7663e0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=300)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(\"안녕하세요?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "xzvPaWZnMRG6",
        "outputId": "61ef764d-9752-4a8e-fc5b-74f18cb63ac0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e172f47a785f>:1: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm.predict(\"안녕하세요?\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요! 👋  무엇을 도와드릴까요? 😊\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 친절하고 유익한 AI 조수입니다. 한국의 역사와 문화에 대해 잘 알고 있습니다.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])"
      ],
      "metadata": {
        "id": "xyOjVepeVG2-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 생성\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "qBrVIXdhVb3N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "# 질문 리스트\n",
        "questions = [\n",
        "    \"한글의 창제 원리는 무엇인가요?\",\n",
        "    \"김치의 역사와 문화적 중요성에 대해 설명해주세요.\",\n",
        "    \"조선시대의 과거 제도에 대해 간단히 설명해주세요.\"\n",
        "]\n",
        "\n",
        "# 각 질문에 대한 답변 생성\n",
        "for question in questions:\n",
        "    response = chain.invoke({\"question\": question})\n",
        "    print(f\"질문: {question}\")\n",
        "    print(f\"답변: {response.content}\\n\") # Use response.content to access the text\n",
        "    print(\"*\" * 150)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZyLdPjwVxb0",
        "outputId": "bf27d1c6-d8e9-4c9d-be8d-0c4eacde1292"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문: 한글의 창제 원리는 무엇인가요?\n",
            "답변: 네, 저는 한국 역사와 문화에 대해 잘 알고 있습니다! 😊\n",
            "\n",
            "한글의 창제 원리는 **세상 모든 소리를 나타내는 글자를 만들고자 하는 탁월한 이념**과 **음성학적 연구를 바탕으로 한 체계적인 구조**에 있습니다. \n",
            "\n",
            "세종대왕께서는 백성들이 쉽게 글을 익힐 수 있도록 **자연의 소리**를 바탕으로 한 새로운 문자를 만들었습니다. \n",
            "\n",
            "* **자음:** 자음은 **입술, 혀, 잇몸 등 발음기관의 모양**을 본떠 만들었습니다.  \n",
            "* **모음:** 모음은 **언어의 모양**과 **발음 범위**를 고려하여 **하늘, 땅, 사람**의 상징을 담아 만들었습니다. \n",
            "\n",
            "각 자음과 모음은 조합하여 다양한 소리를 만들 수 있도록 설계되었고, 이는 **유니크하고 효율적인 문자 체계**를 형성합니다. \n",
            "\n",
            "한글은 단순한 문자를 넘어, **한국 문화와 정신을 담고 있는 훌륭한 유산**입니다. ✨\n",
            "\n",
            "\n",
            "******************************************************************************************************************************************************\n",
            "질문: 김치의 역사와 문화적 중요성에 대해 설명해주세요.\n",
            "답변: ## 김치: 한국의 역사와 문화를 담은 발효 음식\n",
            "\n",
            "김치는 한국의 대표적인 음식이자 문화적 상징물입니다. 단순한 발효 식품을 넘어 한국인의 삶과 역사, 그리고 가치관을 보여주는 중요한 요소입니다.\n",
            "\n",
            "**1. 김치의 역사:**\n",
            "\n",
            "* **고대부터 시작된 발효 문화:** 김치의 역사는 신라 시대까지 거슬러 올라갑니다. 당시에도 배추를 발효시켜 먹었던 것으로 알려져 있으며, 이러한 발효 문화는 북방 유럽의 김치와 닮은 점이 있습니다.\n",
            "* **한반도의 흙과 씨앗:** 김치는 한반도의 특정 기후와 토양에 자생하는 배추, 고추, 젓갈 등의 재료를 활용하여 만들어집니다. 이러한 지역적 특징은 김치의 다양한 맛과 향을 만들어내는 중요한 요인이 되었습니다.\n",
            "* **계절의 흐름 속에서:** 김치는 겨울 동안 먹을 수 있는 저장 식품으로, 농경 사회의 삶과 밀접하게 연결되었습니다. 매기 겨울은 김치를 담그는 중요한 시간이었으며, 가\n",
            "\n",
            "******************************************************************************************************************************************************\n",
            "질문: 조선시대의 과거 제도에 대해 간단히 설명해주세요.\n",
            "답변: 네, 조선시대 과거 제도에 대해 간단히 설명해드릴게요.\n",
            "\n",
            "조선시대 과거는 **지식과 능력을 바탕으로 관직에 임용하는 시험 제도**였습니다. \n",
            "\n",
            "**주요 특징은 다음과 같습니다:**\n",
            "\n",
            "* **임금이 직접 채용:** 과거는 왕이 직접 관직을 채용하는 중요한 제도였습니다. 즉, 일정 기준을 충족하는 사람이라면 누구든 왕에게 직접 관직을 청원할 수 있었습니다.\n",
            "* **4과 3단계 시험:** 과거시험은 사마, 생원, 진사, \n",
            "  어려운 학문을 쌓고 능력을 갖춘 사람들이 관직에 임명될 수 있도록 엄격하고 체계적인 시험 과정을 거쳤습니다. \n",
            "  이를 **사마, 생원, 진사, 급제**라고 부릅니다.\n",
            "* **무게 중심 시험:** 시험은 당시의 유수 학자들이 객관적인 기준으로 평가할 수 있도록 **율곡 이이를 비롯한 학자들이 직접 문제를 만들고 평가**했습니다.\n",
            "* **지방 과거:** 각 지역에서도 과거가 열\n",
            "\n",
            "******************************************************************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: gradio로 만들어줘\n",
        "!pip install gradio langchain-groq --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGtlTlFVVr17",
        "outputId": "5998bcb8-920d-45d3-99ff-fcbaed6cf24d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **gradio**"
      ],
      "metadata": {
        "id": "5HmYnOjdNAh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import gradio as gr\n",
        "\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# ChatGroq 모델 초기화\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=300,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# 프롬프트 템플릿 정의\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 친절하고 유익한 AI 조수입니다. 한국의 역사와 문화에 대해 잘 알고 있습니다.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Chain 생성\n",
        "chain = prompt | llm\n",
        "\n",
        "def predict(message):\n",
        "    response = chain.invoke({\"question\": message})\n",
        "    return response.content\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Korean History & Culture Q&A\",\n",
        "    description=\"Ask me anything about Korean history and culture!\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "8Ty2K_MgdmBp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "a3d3c3ee-3f41-4de6-d139-0f43c2426bec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0c44c33c24d9772e3f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0c44c33c24d9772e3f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# template 정의. {country}는 변수로, 이후에 값이 들어갈 자리를 의미\n",
        "template = \"{country}의 수도는 어디인가요?\"\n",
        "\n",
        "# from_template 메소드를 이용하여 PromptTemplate 객체 생성\n",
        "prompt_template = PromptTemplate.from_template(template) # Change prompt to prompt_template\n",
        "\n",
        "# prompt 생성. format 메소드를 이용하여 변수에 값을 넣어줌\n",
        "prompt_string = prompt_template.format(country=\"대한민국\") # Create a new variable to hold the formatted string\n",
        "\n",
        "\n",
        "# chain 생성\n",
        "chain = prompt_template | llm  # Use the original prompt_template object in the chain"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FCPTlFtKcUBZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6NUhLEhcFvp",
        "outputId": "30525e6b-330e-4d77-a726-602a0340d8fa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')\n",
              "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7c9bf5c95960>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c9bf5c94c10>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=300)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# country 변수에 입력된 값이 자동으로 치환되어 수행됨\n",
        "chain.invoke(\"대한민국\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9X7nIMuochHy",
        "outputId": "36836247-ab0b-405c-81df-6480bde9292e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'대한민국의 수도는 **서울**입니다. 🇰🇷  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"프랑스\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "smJIyqEKNQTi",
        "outputId": "e298cf7d-2f51-4527-a59f-4ad00173339c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'프랑스의 수도는 **파리**입니다. 🇫🇷  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **출력파서(Output Parser)**\n",
        "### **LangChain의 출력파서는 언어 모델(LLM)의 출력을 더 유용하고 구조화된 형태로 변환하는 중요한 컴포넌트**"
      ],
      "metadata": {
        "id": "GhSFT1D_gG6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Fewshot Prompt**"
      ],
      "metadata": {
        "id": "A-bKDWWfdgx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import gradio as gr\n",
        "\n",
        "# Retrieve Groq API key\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# Initialize ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=300,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# Few-Shot Prompting Template for Korean Historical Figures\n",
        "few_shot_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert in providing concise, informative descriptions of Korean historical figures.\n",
        "    Always provide a response in the following format:\n",
        "    - Name: [Full Name]\n",
        "    - Era: [Historical Period]\n",
        "    - Key Achievements: [3-4 most significant contributions]\n",
        "    - Impact: [Lasting influence on Korean history]\"\"\"),\n",
        "\n",
        "    # Few-shot examples to guide the model's response\n",
        "    (\"human\", \"Tell me about King Sejong\"),\n",
        "    (\"ai\", \"\"\"- Name: 세종대왕 (King Sejong the Great)\n",
        "            - Era: Joseon Dynasty (1418-1450)\n",
        "            - Key Achievements:\n",
        "            1. Created Hangul (Korean alphabet)\n",
        "            2. Advanced scientific and cultural development\n",
        "            3. Expanded agricultural techniques\n",
        "            4. Promoted education and scholarship\n",
        "            - Impact: Considered one of the most important monarchs in Korean history, revolutionized communication and cultural understanding\"\"\"),\n",
        "\n",
        "    (\"human\", \"Tell me about Admiral Yi Sun-sin\"),\n",
        "    (\"ai\", \"\"\"- Name: 이순신 (Admiral Yi Sun-sin)\n",
        "            - Era: Joseon Dynasty (Late 16th century)\n",
        "            - Key Achievements:\n",
        "            1. Defended Korea against Japanese invasions\n",
        "            2. Invented the Turtle Ship (Geobukseon)\n",
        "            3. Won 23 consecutive naval battles\n",
        "            4. Exemplified military strategy and leadership\n",
        "            - Impact: National hero who prevented Japanese conquest and saved Korea during the Imjin War\"\"\"),\n",
        "\n",
        "    # The actual query will be added dynamically\n",
        "    (\"human\", \"{historical_figure}\")\n",
        "])\n",
        "\n",
        "# Create the chain\n",
        "chain = few_shot_prompt | llm\n",
        "\n",
        "# Gradio interface function\n",
        "def predict(historical_figure):\n",
        "    response = chain.invoke({\"historical_figure\": historical_figure})\n",
        "    return response.content\n",
        "\n",
        "predict('경복궁')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "EVykw5VBdg5R",
        "outputId": "4d14f5d0-e4d6-4a46-a198-7473e9764809"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"- Name: 경복궁 (Gyeongbokgung)\\n            - Era: Joseon Dynasty (1395-1910)\\n            - Key Achievements:\\n            1. Largest and most magnificent royal palace in Seoul\\n            2. Represents Joseon Dynasty's architectural and artistic grandeur\\n            3. Served as the main residence of Joseon kings\\n            4. Preserved as a UNESCO World Heritage Site\\n            - Impact: A symbol of Korean heritage and a popular tourist destination showcasing traditional Korean architecture and history.\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter a Korean historical figure's name...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Korean Historical Figures Insights\",\n",
        "    description=\"Get structured information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "MLY3_MFYOCM2",
        "outputId": "3608ef37-4b91-4c12-93df-08fce839827d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://96a782355aafdb70b2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://96a782355aafdb70b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. PydanticOuputParser**\n",
        "- **PydanticOutputParser 는 언어 모델의 출력을 더 구조화된 정보로 변환 하는 데 도움이 되는 클래스**\n",
        "- **단순 텍스트 형태의 응답 대신, 사용자가 필요로 하는 정보를 명확하고 체계적인 형태로 제공**"
      ],
      "metadata": {
        "id": "swVoOFbpdh4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "\n",
        "# Retrieve Groq API key\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# Define a Pydantic model for structured historical figure information\n",
        "class HistoricalFigure(BaseModel):\n",
        "    name: str = Field(description=\"Full name of the historical figure\")\n",
        "    korean_name: str = Field(description=\"Name in Korean characters\")\n",
        "    birth_year: int = Field(description=\"Year of birth\")\n",
        "    death_year: int = Field(description=\"Year of death\")\n",
        "    era: str = Field(description=\"Historical period\")\n",
        "    key_achievements: List[str] = Field(description=\"3-4 most significant contributions\")\n",
        "    impact: str = Field(description=\"Lasting influence on Korean history\")\n",
        "    interesting_fact: str = Field(description=\"A unique or surprising detail about the figure\")\n",
        "\n",
        "# Initialize ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# Create a PydanticOutputParser\n",
        "parser = PydanticOutputParser(pydantic_object=HistoricalFigure)\n",
        "\n",
        "# Create a prompt template that includes output instructions\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert historian specializing in Korean history.\n",
        "    Provide detailed, accurate information about historical figures.\n",
        "\n",
        "    {format_instructions}\n",
        "\n",
        "    Please provide comprehensive information about the requested historical figure.\"\"\"),\n",
        "    (\"human\", \"{historical_figure}\")\n",
        "])\n",
        "\n",
        "# Combine the prompt, parsing instructions, and model\n",
        "chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm | parser\n",
        "\n",
        "# Gradio interface function\n",
        "def get_historical_figure_info(figure_name):\n",
        "    try:\n",
        "        result = chain.invoke({\"historical_figure\": figure_name})\n",
        "        # Convert Pydantic model to a formatted string\n",
        "        return \"\\n\".join([\n",
        "            f\"**Name:** {result.name} ({result.korean_name})\",\n",
        "            f\"**Lived:** {result.birth_year} - {result.death_year}\",\n",
        "            f\"**Era:** {result.era}\",\n",
        "            \"**Key Achievements:**\",\n",
        "            *[f\"- {achievement}\" for achievement in result.key_achievements],\n",
        "            f\"\\n**Historical Impact:** {result.impact}\",\n",
        "            f\"\\n**Interesting Fact:** {result.interesting_fact}\"\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=get_historical_figure_info,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter a Korean historical figure's name...\"),\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Structured Korean Historical Figures\",\n",
        "    description=\"Get detailed, structured information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "XY7_WtRTer4J",
        "outputId": "42903696-5bb5-40b5-c497-f6a1f99c2ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a9d914bd1a0c2bb24a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a9d914bd1a0c2bb24a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. JsonOutputParser**\n",
        "- **출력 파서는 사용자가 원하는 JSON 스키마를 지정할 수 있게 해주며, 그 스키마에 맞게 LLM에서 데이터를 조회하여 결과를 도출**\n",
        "- **LLM이 데이터를 정확하고 효율적으로 처리하여 원하는 형태의 JSON을 생성하기 위해서는, 모델의 용량(여기서는 인텔리전스를 의미**\n",
        "- **예. llama-70B 이 llama-8B 보다 용량이 크다) 이 충분해야 한다는 점을 참고**"
      ],
      "metadata": {
        "id": "pQ4ILdl8euii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "import json\n",
        "\n",
        "# Retrieve Groq API key\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# Initialize ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# Create a JsonOutputParser\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "# Create a prompt template that includes JSON output instructions\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert historian specializing in Korean history.\n",
        "    Provide detailed, accurate information about historical figures in a strict JSON format.\n",
        "\n",
        "    Respond with a JSON object containing the following keys:\n",
        "    - name: Full name of the historical figure\n",
        "    - korean_name: Name in Korean characters\n",
        "    - birth_year: Year of birth (integer)\n",
        "    - death_year: Year of death (integer)\n",
        "    - era: Historical period\n",
        "    - key_achievements: List of most significant contributions\n",
        "    - impact: Lasting influence on Korean history\n",
        "    - interesting_fact: A unique or surprising detail about the figure\n",
        "\n",
        "    {format_instructions}\"\"\"),\n",
        "    (\"human\", \"Tell me about {historical_figure}\")\n",
        "])\n",
        "\n",
        "# Combine the prompt, parsing instructions, and model\n",
        "chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm | parser\n",
        "\n",
        "# Gradio interface function\n",
        "def get_historical_figure_info(figure_name):\n",
        "    try:\n",
        "        result = chain.invoke({\"historical_figure\": figure_name})\n",
        "\n",
        "        # Format the JSON result as a readable markdown string\n",
        "        formatted_output = \"\\n\".join([\n",
        "            f\"**Name:** {result.get('name', 'N/A')} ({result.get('korean_name', 'N/A')})\",\n",
        "            f\"**Lived:** {result.get('birth_year', 'N/A')} - {result.get('death_year', 'N/A')}\",\n",
        "            f\"**Era:** {result.get('era', 'N/A')}\",\n",
        "            \"**Key Achievements:**\",\n",
        "            *[f\"- {achievement}\" for achievement in result.get('key_achievements', [])],\n",
        "            f\"\\n**Historical Impact:** {result.get('impact', 'N/A')}\",\n",
        "            f\"\\n**Interesting Fact:** {result.get('interesting_fact', 'N/A')}\"\n",
        "        ])\n",
        "\n",
        "        # Also return the raw JSON for reference\n",
        "        return formatted_output + f\"\\n\\n**Raw JSON:**\\n```json\\n{json.dumps(result, indent=2, ensure_ascii=False)}```\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=get_historical_figure_info,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter a Korean historical figure's name...\"),\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Structured Korean Historical Figures (JSON)\",\n",
        "    description=\"Get detailed, structured JSON information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "lqCGFhvZgEY_",
        "outputId": "d0be6a49-169c-44a5-d8ea-131c48b8b31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eced003eac4134965f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eced003eac4134965f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. PandasDataFrameOutputParser**\n",
        "- **Pandas DataFrame은 Python 프로그래밍 언어에서 널리 사용되는 데이터 구조로, 데이터 조작 및 분석을 위해 흔히 사용되며 구조화된 데이터를 다루기 위한 포괄적인 도구 세트를 제공하여, 데이터 정제, 변환 및 분석과 같은 작업에 다양하게 활용**"
      ],
      "metadata": {
        "id": "2-0T-uYug6Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "# 예시 데이터를 사용하여 데이터 프레임 생성 함수 정의\n",
        "def get_historical_figures(figure_name=None):\n",
        "    # 한국 역사적 인물들에 대한 예시 데이터\n",
        "    figures_list = [\n",
        "        {\n",
        "            \"name\": \"Yi Sun-sin\",\n",
        "            \"korean_name\": \"이순신\",\n",
        "            \"birth_year\": 1545,\n",
        "            \"death_year\": 1598,\n",
        "            \"era\": \"Joseon Dynasty\",\n",
        "            \"primary_role\": \"Admiral\",\n",
        "            \"key_achievement\": \"Defeated Japanese Navy during the Imjin War\",\n",
        "            \"historical_significance\": \"National hero known for his naval victories against Japan\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Sejong the Great\",\n",
        "            \"korean_name\": \"세종대왕\",\n",
        "            \"birth_year\": 1397,\n",
        "            \"death_year\": 1450,\n",
        "            \"era\": \"Joseon Dynasty\",\n",
        "            \"primary_role\": \"King of Joseon\",\n",
        "            \"key_achievement\": \"Created the Korean alphabet Hangul\",\n",
        "            \"historical_significance\": \"One of the most respected kings, greatly improved Korean culture and literacy\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Kim Gu\",\n",
        "            \"korean_name\": \"김구\",\n",
        "            \"birth_year\": 1876,\n",
        "            \"death_year\": 1949,\n",
        "            \"era\": \"Korean Empire / Japanese Occupation\",\n",
        "            \"primary_role\": \"Politician\",\n",
        "            \"key_achievement\": \"Leader of the Korean independence movement\",\n",
        "            \"historical_significance\": \"Major figure in the movement for Korean independence from Japan\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # 데이터를 pandas DataFrame으로 변환\n",
        "    df = pd.DataFrame(figures_list)\n",
        "\n",
        "    # 특정 인물 이름을 입력받았을 경우 필터링\n",
        "    if figure_name:\n",
        "        df = df[(df['korean_name'] == figure_name) | (df['name'] == figure_name)]\n",
        "\n",
        "    # 데이터가 없을 경우 메시지를 담은 DataFrame 반환\n",
        "    if df.empty:\n",
        "        return pd.DataFrame([{\"Message\": f\"No information found for {figure_name}\"}])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Gradio 인터페이스 정의\n",
        "iface = gr.Interface(\n",
        "    fn=get_historical_figures,\n",
        "    inputs=\"text\",  # 특정 역사적 인물 이름을 입력받기 위해 텍스트 입력 사용\n",
        "    outputs=\"dataframe\",  # 데이터 프레임을 출력\n",
        "    title=\"Structured Korean Historical Figures (Pandas DataFrame)\",\n",
        "    description=\"Get detailed, structured information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# 인터페이스 실행\n",
        "iface.launch()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "VnXGlfYvg_3x",
        "outputId": "bd3566ba-e8ba-478f-b629-bcdad01d3d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9b7153e0b8d38cf412.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9b7153e0b8d38cf412.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_core.output_parsers as parsers\n",
        "print(dir(parsers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc1hYTW3hSsR",
        "outputId": "86b709f1-6e37-4522-b730-f15477868d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BaseCumulativeTransformOutputParser', 'BaseGenerationOutputParser', 'BaseLLMOutputParser', 'BaseOutputParser', 'BaseTransformOutputParser', 'CommaSeparatedListOutputParser', 'JsonOutputKeyToolsParser', 'JsonOutputParser', 'JsonOutputToolsParser', 'ListOutputParser', 'MarkdownListOutputParser', 'NumberedListOutputParser', 'PydanticOutputParser', 'PydanticToolsParser', 'SimpleJsonOutputParser', 'StrOutputParser', 'XMLOutputParser', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'base', 'format_instructions', 'json', 'list', 'openai_tools', 'pydantic', 'string', 'transform', 'xml']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnUM5Gt4kKV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0dVkaBjiYz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}