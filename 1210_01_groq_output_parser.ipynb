{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhRljuBFjG42ZFuYZ0dtb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/24_fall_textmining_NLP/blob/main/1210_01_groq_output_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Groq API](https://wikidocs.net/259655)**\n",
        "\n",
        "- https://console.groq.com/playground\n",
        "- https://python.langchain.com/docs/how_to/sequence/ **(Langchain Tutorial)**\n",
        "- https://wikidocs.net/book/14314 **(í•œê¸€íŒ ë‘ì²´ì¸ íŠœí† ë¦¬ì–¼)**"
      ],
      "metadata": {
        "id": "UmjMoIobUZr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_key = userdata.get('groq')"
      ],
      "metadata": {
        "id": "GuxZqRXkU-aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-dOigfFVNyr",
        "outputId": "efa79afb-8a04-4151-b075-05a1f93a420e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ptXYOaAUUAR"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# ChatGroq ëª¨ë¸ ì´ˆê¸°í™”\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\", # google/gemma-2-9b-it\n",
        "    temperature=0.7,\n",
        "    max_tokens=300,\n",
        "    api_key=groq_key\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ìµí•œ AI ì¡°ìˆ˜ì…ë‹ˆë‹¤. í•œêµ­ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆìŠµë‹ˆë‹¤.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])"
      ],
      "metadata": {
        "id": "xyOjVepeVG2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain ìƒì„±\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "qBrVIXdhVb3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
        "questions = [\n",
        "    \"í•œê¸€ì˜ ì°½ì œ ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "    \"ê¹€ì¹˜ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ì  ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
        "    \"ì¡°ì„ ì‹œëŒ€ì˜ ê³¼ê±° ì œë„ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "]\n",
        "\n",
        "# ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
        "for question in questions:\n",
        "    response = chain.invoke({\"question\": question})\n",
        "    print(f\"ì§ˆë¬¸: {question}\")\n",
        "    print(f\"ë‹µë³€: {response.content}\\n\") # Use response.content to access the text"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZyLdPjwVxb0",
        "outputId": "f20fdda3-0e84-4209-f4c9-311b67533d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì§ˆë¬¸: í•œê¸€ì˜ ì°½ì œ ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
            "ë‹µë³€: í•œê¸€ì˜ ì°½ì œ ì›ë¦¬ëŠ” ë§¤ìš° í¥ë¯¸ë¡­ìŠµë‹ˆë‹¤! \n",
            "\n",
            "ì„¸ì¢…ëŒ€ì™•ì€ ë°±ì„±ë“¤ì´ ê¸€ì„ ì½ê³  ì¨ ì•Œ ìˆ˜ ìˆë„ë¡ **\"ì‚¬ëŒì˜ ë°œìŒê³¼ ì†Œë¦¬\"**ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê¸€ì ì²´ê³„ë¥¼ ë§Œë“¤ê³ ì í–ˆìŠµë‹ˆë‹¤. \n",
            "\n",
            "**ë‹¤ìŒì€ í•œê¸€ ì°½ì œì˜ í•µì‹¬ ì›ë¦¬ì…ë‹ˆë‹¤:**\n",
            "\n",
            "* **ììŒê³¼ ëª¨ìŒì˜ ë¶„ë¦¬:** í•œê¸€ì€ ììŒê³¼ ëª¨ìŒì„ ë¶„ë¦¬í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ììŒì€ ë°œìŒì˜ ìˆ˜ë‹¨(ì˜ˆ: í˜€, ì…ìˆ , ê¸°ì¹¨)ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì—ˆê³ , ëª¨ìŒì€ ìŒì„± ê³µê°„(ì˜ˆ: ì… ëª¨ì–‘)ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.\n",
            "* **ìŒì„±í•™ì  ë¶„ì„:** í•œê¸€ì€ ë°œìŒì˜ ì›ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ììŒê³¼ ëª¨ìŒì„ í‘œí˜„í•˜ëŠ” ê¸°í˜¸ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. \n",
            "* **ìŒì„± ê¸°í˜¸ì˜ í‘œí˜„:** ììŒì€ ë°œìŒì„ í•˜ëŠ” ê¸°ê´€ì˜ ì›€ì§ì„ì„, ëª¨ìŒì€ ë°œìŒ ì‹œ ëª¨ì–‘ì„ ì§ì ‘ì ìœ¼ë¡œ í‘œí˜„í•˜ë©°, ì´ëŸ¬í•œ ê¸°í˜¸ë“¤ì´ ì¡°í•©ë  ë•Œ ë‹¤ì–‘í•œ ì†Œë¦¬ë¥¼ ë§Œë“¤ì–´ë‚´ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "**ì˜ˆë¥¼ ë“¤ì–´, \"ã„±\"ì€ ê¸°ì¹¨ì†Œë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ì´ê³ , \"ã…\"ëŠ” ì…ìˆ  ëª¨ì–‘ì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ì…ë‹ˆë‹¤.** ì´ëŸ¬í•œ ê¸°\n",
            "\n",
            "ì§ˆë¬¸: ê¹€ì¹˜ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ì  ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ## ê¹€ì¹˜: í•œêµ­ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ë¥¼ ë‹´ì€ ë§›\n",
            "\n",
            "ê¹€ì¹˜ëŠ” í•œêµ­ì¸ì˜ ì‚¶ì—ì„œ ë—„ ìˆ˜ ì—†ëŠ” ì‹íƒì˜ ì£¼ì¸ê³µì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ìŒì‹ì„ ë„˜ì–´ í•œêµ­ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ë¥¼ ë°˜ì˜í•˜ëŠ” ì¤‘ìš”í•œ ë¬¸í™”ìœ ì‚°ì´ì£ . \n",
            "\n",
            "**ì—­ì‚¬:**\n",
            "\n",
            "* ê¹€ì¹˜ì˜ ì—­ì‚¬ëŠ” ë§¤ìš° ì˜¤ë˜ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì›ì „ 2ì²œë…„ê²½ ê±°ì³ì˜¨ ì‚¼êµ­ì‹œëŒ€ë¶€í„° ê¹€ì¹˜ë¥¼ ì¦ê²¼ë‹¤ëŠ” ê¸°ë¡ì´ ë‚¨ì•„ìˆìœ¼ë©°, ë‹¹ë‚˜ë¼ì˜ ê²½ìš°ë„ ê¹€ì¹˜ benzeri fermente ëœ ìŒì‹ì„ ì¦ê²¼ë‹¤ëŠ” ê¸°ë¡ì´ ìˆìŠµë‹ˆë‹¤.\n",
            "* 10ì„¸ê¸°ë¶€í„°ëŠ” êµ­ë‚´ ì¬ë˜ê¹€ì¹˜ ì–‘ì¡° ê¸°ìˆ ì´ ë°œì „í•˜ê³  ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê¹€ì¹˜ê°€ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.\n",
            "* ì¡°ì„ ì‹œëŒ€ì—ëŠ” ê¹€ì¹˜ê°€ êµ­ê°€ì ìœ¼ë¡œ ì¤‘ìš”í•˜ê²Œ ì—¬ê²¨ì¡ŒìŠµë‹ˆë‹¤. ê³¡ì‹ì´ ë¶€ì¡±í•  ë•Œë‚˜ ê²¨ìš¸ì²  ì‹ëŸ‰ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê¹€ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” ë¬¸í™”ê°€ ì‹œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤. \n",
            "* ìŒ€ê³¼ í•¨ê»˜ ê¹€ì¹˜ëŠ” í•œêµ­ì¸ì˜ ì£¼ì‹ì´ ë˜ì—ˆê³ , íƒ„íƒ„í•œ ì‹ëŸ‰ ê¸°ë°˜ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "**ë¬¸í™”ì  ì¤‘ìš”ì„±:**\n",
            "\n",
            "* **ê±´ê°•:** ê¹€ì¹˜ëŠ” ìœ ì‚°ê· ì´ í’ë¶€í•˜ì—¬ ì†Œí™”ë¥¼ ë•ê³  ë©´ì—­ë ¥ì„ ê°•í™”í•˜ëŠ” íš¨ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤. \n",
            "\n",
            "ì§ˆë¬¸: ì¡°ì„ ì‹œëŒ€ì˜ ê³¼ê±° ì œë„ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
            "ë‹µë³€: ì¡°ì„ ì‹œëŒ€ ê³¼ê±° ì œë„ëŠ” **í•œì„±ì„ ê¿ˆê¾¸ëŠ” ì²­ë…„ë“¤ì˜ ì—´ì •ê³¼ ì§€í˜œë¥¼ ì‹œí—˜í•˜ëŠ”, ì—„ê²©í•˜ë©´ì„œë„ ì •ì¹˜ì  ì¤‘ì‹¬ì„ ì´ë£¨ëŠ” ì‹œìŠ¤í…œ**ì´ì—ˆìŠµë‹ˆë‹¤. \n",
            "\n",
            "**í•µì‹¬ì€ 'ê³µì •í•œ ê¸°íšŒ'ì™€ 'ëŠ¥ë ¥ ì¸ì •'**ì— ìˆì—ˆì£ ! \n",
            "\n",
            "* **ëˆ„êµ¬ë“  ì‹œí—˜ì„ í†µí•´ ê´€ë£Œê°€ ë  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.** \n",
            "  * ì²œë¯¼ì´ë¼ë„ ë›°ì–´ë‚œ í•™ì—… ì„±ì·¨ë¥¼ í†µí•´  'ì£¼ê´€', 'ì‹œí—˜ê´€' ë“± ë†’ì€ ì§ìœ„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆì—ˆì£ .\n",
            "* **ê³¼ê±° ì‹œí—˜ì€ ë§¤ìš° ì–´ë ¤ì› ìŠµë‹ˆë‹¤.** \n",
            "  * 4ê°œì˜ ë‹¨ê³„ë¥¼ ê±°ì³ ë©´ì ‘(ì‹œí—˜)ê³¼ ë‹µë³€(ì˜ˆìˆ ë¬¸)ì„ í†µí•´ ëŠ¥ë ¥ì„ ìˆ™ë‹¬í•´ì•¼ í–ˆìŠµë‹ˆë‹¤.\n",
            "* **ê³¼ê±°ëŠ” ì¡°ì„  ì‹œëŒ€ì˜ ê¶ê·¹ì ì¸ ì‚¬íšŒì  ì´ë™ ì‚¬ìŠ¬**ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
            "  * ê³¼ê±°ì— í•©ê²©í•˜ë©´ ê´€ë£Œê°€ ë˜ê³ , ì‚¬íšŒì ìœ¼ë¡œ ë†’ì€ ì§€ìœ„ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ê³¼ê±° ì‹œí—˜ì€ ë‹¨ìˆœíˆ ì§€ì‹ì„ ë¬»ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ì¡°ì„  ì‚¬íšŒì˜ ê°€ì¹˜ê´€ê³¼ ìœ¤ë¦¬, ì •ì¹˜ì  ì‚¬ìƒ**ê¹Œì§€ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œì´ì—ˆìŠµë‹ˆë‹¤. \n",
            "\n",
            "ì¡°ì„ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: gradioë¡œ ë§Œë“¤ì–´ì¤˜\n",
        "\n",
        "!pip install gradio langchain-groq --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGtlTlFVVr17",
        "outputId": "e62679db-b88a-4707-921e-51d4f322a738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import gradio as gr\n",
        "\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# ChatGroq ëª¨ë¸ ì´ˆê¸°í™”\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=300,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ìµí•œ AI ì¡°ìˆ˜ì…ë‹ˆë‹¤. í•œêµ­ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆìŠµë‹ˆë‹¤.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Chain ìƒì„±\n",
        "chain = prompt | llm\n",
        "\n",
        "def predict(message):\n",
        "    response = chain.invoke({\"question\": message})\n",
        "    return response.content\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Korean History & Culture Q&A\",\n",
        "    description=\"Ask me anything about Korean history and culture!\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "8Ty2K_MgdmBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRqW9QKTV9HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# template ì •ì˜. {country}ëŠ” ë³€ìˆ˜ë¡œ, ì´í›„ì— ê°’ì´ ë“¤ì–´ê°ˆ ìë¦¬ë¥¼ ì˜ë¯¸\n",
        "template = \"{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\"\n",
        "\n",
        "# from_template ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ì—¬ PromptTemplate ê°ì²´ ìƒì„±\n",
        "prompt_template = PromptTemplate.from_template(template) # Change prompt to prompt_template\n",
        "\n",
        "# prompt ìƒì„±. format ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ì—¬ ë³€ìˆ˜ì— ê°’ì„ ë„£ì–´ì¤Œ\n",
        "prompt_string = prompt_template.format(country=\"ëŒ€í•œë¯¼êµ­\") # Create a new variable to hold the formatted string\n",
        "\n",
        "\n",
        "# chain ìƒì„±\n",
        "chain = prompt_template | llm  # Use the original prompt_template object in the chain"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FCPTlFtKcUBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6NUhLEhcFvp",
        "outputId": "806564e5-da84-4485-fa9e-663474a1fd0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?')\n",
              "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e6f5fcf63b0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e6f5fb12350>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=300)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# country ë³€ìˆ˜ì— ì…ë ¥ëœ ê°’ì´ ìë™ìœ¼ë¡œ ì¹˜í™˜ë˜ì–´ ìˆ˜í–‰ë¨\n",
        "chain.invoke(\"ëŒ€í•œë¯¼êµ­\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9X7nIMuochHy",
        "outputId": "0138d3c8-256f-42ec-a1bd-588751538b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” **ì„œìš¸**ì…ë‹ˆë‹¤. ğŸ˜Š \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì¶œë ¥íŒŒì„œ(Output Parser)**\n",
        "### **LangChainì˜ ì¶œë ¥íŒŒì„œëŠ” ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì¶œë ¥ì„ ë” ìœ ìš©í•˜ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ì¤‘ìš”í•œ ì»´í¬ë„ŒíŠ¸**"
      ],
      "metadata": {
        "id": "GhSFT1D_gG6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Fewshot Prompt**"
      ],
      "metadata": {
        "id": "A-bKDWWfdgx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import gradio as gr\n",
        "\n",
        "# Retrieve Groq API key\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# Initialize ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=300,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# Few-Shot Prompting Template for Korean Historical Figures\n",
        "few_shot_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert in providing concise, informative descriptions of Korean historical figures.\n",
        "    Always provide a response in the following format:\n",
        "    - Name: [Full Name]\n",
        "    - Era: [Historical Period]\n",
        "    - Key Achievements: [3-4 most significant contributions]\n",
        "    - Impact: [Lasting influence on Korean history]\"\"\"),\n",
        "\n",
        "    # Few-shot examples to guide the model's response\n",
        "    (\"human\", \"Tell me about King Sejong\"),\n",
        "    (\"ai\", \"\"\"- Name: ì„¸ì¢…ëŒ€ì™• (King Sejong the Great)\n",
        "- Era: Joseon Dynasty (1418-1450)\n",
        "- Key Achievements:\n",
        "  1. Created Hangul (Korean alphabet)\n",
        "  2. Advanced scientific and cultural development\n",
        "  3. Expanded agricultural techniques\n",
        "  4. Promoted education and scholarship\n",
        "- Impact: Considered one of the most important monarchs in Korean history, revolutionized communication and cultural understanding\"\"\"),\n",
        "\n",
        "    (\"human\", \"Tell me about Admiral Yi Sun-sin\"),\n",
        "    (\"ai\", \"\"\"- Name: ì´ìˆœì‹  (Admiral Yi Sun-sin)\n",
        "- Era: Joseon Dynasty (Late 16th century)\n",
        "- Key Achievements:\n",
        "  1. Defended Korea against Japanese invasions\n",
        "  2. Invented the Turtle Ship (Geobukseon)\n",
        "  3. Won 23 consecutive naval battles\n",
        "  4. Exemplified military strategy and leadership\n",
        "- Impact: National hero who prevented Japanese conquest and saved Korea during the Imjin War\"\"\"),\n",
        "\n",
        "    # The actual query will be added dynamically\n",
        "    (\"human\", \"{historical_figure}\")\n",
        "])\n",
        "\n",
        "# Create the chain\n",
        "chain = few_shot_prompt | llm\n",
        "\n",
        "# Gradio interface function\n",
        "def predict(historical_figure):\n",
        "    response = chain.invoke({\"historical_figure\": historical_figure})\n",
        "    return response.content\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter a Korean historical figure's name...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Korean Historical Figures Insights\",\n",
        "    description=\"Get structured information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "EVykw5VBdg5R",
        "outputId": "df4a9934-35f5-4267-ead7-e0db64ba16b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d4ac511d2f04f87bde.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d4ac511d2f04f87bde.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. PydanticOuputParser**\n",
        "- **PydanticOutputParser ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ë” êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ ë³€í™˜ í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” í´ë˜ìŠ¤**\n",
        "- **ë‹¨ìˆœ í…ìŠ¤íŠ¸ í˜•íƒœì˜ ì‘ë‹µ ëŒ€ì‹ , ì‚¬ìš©ìê°€ í•„ìš”ë¡œ í•˜ëŠ” ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ì²´ê³„ì ì¸ í˜•íƒœë¡œ ì œê³µ**"
      ],
      "metadata": {
        "id": "swVoOFbpdh4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "\n",
        "# Retrieve Groq API key\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# Define a Pydantic model for structured historical figure information\n",
        "class HistoricalFigure(BaseModel):\n",
        "    name: str = Field(description=\"Full name of the historical figure\")\n",
        "    korean_name: str = Field(description=\"Name in Korean characters\")\n",
        "    birth_year: int = Field(description=\"Year of birth\")\n",
        "    death_year: int = Field(description=\"Year of death\")\n",
        "    era: str = Field(description=\"Historical period\")\n",
        "    key_achievements: List[str] = Field(description=\"3-4 most significant contributions\")\n",
        "    impact: str = Field(description=\"Lasting influence on Korean history\")\n",
        "    interesting_fact: str = Field(description=\"A unique or surprising detail about the figure\")\n",
        "\n",
        "# Initialize ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# Create a PydanticOutputParser\n",
        "parser = PydanticOutputParser(pydantic_object=HistoricalFigure)\n",
        "\n",
        "# Create a prompt template that includes output instructions\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert historian specializing in Korean history.\n",
        "    Provide detailed, accurate information about historical figures.\n",
        "\n",
        "    {format_instructions}\n",
        "\n",
        "    Please provide comprehensive information about the requested historical figure.\"\"\"),\n",
        "    (\"human\", \"{historical_figure}\")\n",
        "])\n",
        "\n",
        "# Combine the prompt, parsing instructions, and model\n",
        "chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm | parser\n",
        "\n",
        "# Gradio interface function\n",
        "def get_historical_figure_info(figure_name):\n",
        "    try:\n",
        "        result = chain.invoke({\"historical_figure\": figure_name})\n",
        "        # Convert Pydantic model to a formatted string\n",
        "        return \"\\n\".join([\n",
        "            f\"**Name:** {result.name} ({result.korean_name})\",\n",
        "            f\"**Lived:** {result.birth_year} - {result.death_year}\",\n",
        "            f\"**Era:** {result.era}\",\n",
        "            \"**Key Achievements:**\",\n",
        "            *[f\"- {achievement}\" for achievement in result.key_achievements],\n",
        "            f\"\\n**Historical Impact:** {result.impact}\",\n",
        "            f\"\\n**Interesting Fact:** {result.interesting_fact}\"\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=get_historical_figure_info,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter a Korean historical figure's name...\"),\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Structured Korean Historical Figures\",\n",
        "    description=\"Get detailed, structured information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "XY7_WtRTer4J",
        "outputId": "42903696-5bb5-40b5-c497-f6a1f99c2ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a9d914bd1a0c2bb24a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a9d914bd1a0c2bb24a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. JsonOutputParser**\n",
        "- **ì¶œë ¥ íŒŒì„œëŠ” ì‚¬ìš©ìê°€ ì›í•˜ëŠ” JSON ìŠ¤í‚¤ë§ˆë¥¼ ì§€ì •í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°, ê·¸ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ LLMì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ ê²°ê³¼ë¥¼ ë„ì¶œ**\n",
        "- **LLMì´ ë°ì´í„°ë¥¼ ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì›í•˜ëŠ” í˜•íƒœì˜ JSONì„ ìƒì„±í•˜ê¸° ìœ„í•´ì„œëŠ”, ëª¨ë¸ì˜ ìš©ëŸ‰(ì—¬ê¸°ì„œëŠ” ì¸í…”ë¦¬ì „ìŠ¤ë¥¼ ì˜ë¯¸**\n",
        "- **ì˜ˆ. llama-70B ì´ llama-8B ë³´ë‹¤ ìš©ëŸ‰ì´ í¬ë‹¤) ì´ ì¶©ë¶„í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ì°¸ê³ **"
      ],
      "metadata": {
        "id": "pQ4ILdl8euii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "import json\n",
        "\n",
        "# Retrieve Groq API key\n",
        "groq_key = userdata.get('groq')\n",
        "\n",
        "# Initialize ChatGroq model\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    api_key=groq_key\n",
        ")\n",
        "\n",
        "# Create a JsonOutputParser\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "# Create a prompt template that includes JSON output instructions\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert historian specializing in Korean history.\n",
        "    Provide detailed, accurate information about historical figures in a strict JSON format.\n",
        "\n",
        "    Respond with a JSON object containing the following keys:\n",
        "    - name: Full name of the historical figure\n",
        "    - korean_name: Name in Korean characters\n",
        "    - birth_year: Year of birth (integer)\n",
        "    - death_year: Year of death (integer)\n",
        "    - era: Historical period\n",
        "    - key_achievements: List of most significant contributions\n",
        "    - impact: Lasting influence on Korean history\n",
        "    - interesting_fact: A unique or surprising detail about the figure\n",
        "\n",
        "    {format_instructions}\"\"\"),\n",
        "    (\"human\", \"Tell me about {historical_figure}\")\n",
        "])\n",
        "\n",
        "# Combine the prompt, parsing instructions, and model\n",
        "chain = prompt.partial(format_instructions=parser.get_format_instructions()) | llm | parser\n",
        "\n",
        "# Gradio interface function\n",
        "def get_historical_figure_info(figure_name):\n",
        "    try:\n",
        "        result = chain.invoke({\"historical_figure\": figure_name})\n",
        "\n",
        "        # Format the JSON result as a readable markdown string\n",
        "        formatted_output = \"\\n\".join([\n",
        "            f\"**Name:** {result.get('name', 'N/A')} ({result.get('korean_name', 'N/A')})\",\n",
        "            f\"**Lived:** {result.get('birth_year', 'N/A')} - {result.get('death_year', 'N/A')}\",\n",
        "            f\"**Era:** {result.get('era', 'N/A')}\",\n",
        "            \"**Key Achievements:**\",\n",
        "            *[f\"- {achievement}\" for achievement in result.get('key_achievements', [])],\n",
        "            f\"\\n**Historical Impact:** {result.get('impact', 'N/A')}\",\n",
        "            f\"\\n**Interesting Fact:** {result.get('interesting_fact', 'N/A')}\"\n",
        "        ])\n",
        "\n",
        "        # Also return the raw JSON for reference\n",
        "        return formatted_output + f\"\\n\\n**Raw JSON:**\\n```json\\n{json.dumps(result, indent=2, ensure_ascii=False)}```\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=get_historical_figure_info,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter a Korean historical figure's name...\"),\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Structured Korean Historical Figures (JSON)\",\n",
        "    description=\"Get detailed, structured JSON information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "lqCGFhvZgEY_",
        "outputId": "d0be6a49-169c-44a5-d8ea-131c48b8b31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eced003eac4134965f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eced003eac4134965f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. PandasDataFrameOutputParser**\n",
        "- **Pandas DataFrameì€ Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° êµ¬ì¡°ë¡œ, ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•´ í”íˆ ì‚¬ìš©ë˜ë©° êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ í¬ê´„ì ì¸ ë„êµ¬ ì„¸íŠ¸ë¥¼ ì œê³µí•˜ì—¬, ë°ì´í„° ì •ì œ, ë³€í™˜ ë° ë¶„ì„ê³¼ ê°™ì€ ì‘ì—…ì— ë‹¤ì–‘í•˜ê²Œ í™œìš©**"
      ],
      "metadata": {
        "id": "2-0T-uYug6Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "# ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í”„ë ˆì„ ìƒì„± í•¨ìˆ˜ ì •ì˜\n",
        "def get_historical_figures(figure_name=None):\n",
        "    # í•œêµ­ ì—­ì‚¬ì  ì¸ë¬¼ë“¤ì— ëŒ€í•œ ì˜ˆì‹œ ë°ì´í„°\n",
        "    figures_list = [\n",
        "        {\n",
        "            \"name\": \"Yi Sun-sin\",\n",
        "            \"korean_name\": \"ì´ìˆœì‹ \",\n",
        "            \"birth_year\": 1545,\n",
        "            \"death_year\": 1598,\n",
        "            \"era\": \"Joseon Dynasty\",\n",
        "            \"primary_role\": \"Admiral\",\n",
        "            \"key_achievement\": \"Defeated Japanese Navy during the Imjin War\",\n",
        "            \"historical_significance\": \"National hero known for his naval victories against Japan\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Sejong the Great\",\n",
        "            \"korean_name\": \"ì„¸ì¢…ëŒ€ì™•\",\n",
        "            \"birth_year\": 1397,\n",
        "            \"death_year\": 1450,\n",
        "            \"era\": \"Joseon Dynasty\",\n",
        "            \"primary_role\": \"King of Joseon\",\n",
        "            \"key_achievement\": \"Created the Korean alphabet Hangul\",\n",
        "            \"historical_significance\": \"One of the most respected kings, greatly improved Korean culture and literacy\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Kim Gu\",\n",
        "            \"korean_name\": \"ê¹€êµ¬\",\n",
        "            \"birth_year\": 1876,\n",
        "            \"death_year\": 1949,\n",
        "            \"era\": \"Korean Empire / Japanese Occupation\",\n",
        "            \"primary_role\": \"Politician\",\n",
        "            \"key_achievement\": \"Leader of the Korean independence movement\",\n",
        "            \"historical_significance\": \"Major figure in the movement for Korean independence from Japan\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # ë°ì´í„°ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "    df = pd.DataFrame(figures_list)\n",
        "\n",
        "    # íŠ¹ì • ì¸ë¬¼ ì´ë¦„ì„ ì…ë ¥ë°›ì•˜ì„ ê²½ìš° í•„í„°ë§\n",
        "    if figure_name:\n",
        "        df = df[(df['korean_name'] == figure_name) | (df['name'] == figure_name)]\n",
        "\n",
        "    # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ë©”ì‹œì§€ë¥¼ ë‹´ì€ DataFrame ë°˜í™˜\n",
        "    if df.empty:\n",
        "        return pd.DataFrame([{\"Message\": f\"No information found for {figure_name}\"}])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Gradio ì¸í„°í˜ì´ìŠ¤ ì •ì˜\n",
        "iface = gr.Interface(\n",
        "    fn=get_historical_figures,\n",
        "    inputs=\"text\",  # íŠ¹ì • ì—­ì‚¬ì  ì¸ë¬¼ ì´ë¦„ì„ ì…ë ¥ë°›ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ ì…ë ¥ ì‚¬ìš©\n",
        "    outputs=\"dataframe\",  # ë°ì´í„° í”„ë ˆì„ì„ ì¶œë ¥\n",
        "    title=\"Structured Korean Historical Figures (Pandas DataFrame)\",\n",
        "    description=\"Get detailed, structured information about significant people in Korean history\"\n",
        ")\n",
        "\n",
        "# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
        "iface.launch()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "VnXGlfYvg_3x",
        "outputId": "bd3566ba-e8ba-478f-b629-bcdad01d3d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9b7153e0b8d38cf412.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9b7153e0b8d38cf412.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_core.output_parsers as parsers\n",
        "print(dir(parsers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc1hYTW3hSsR",
        "outputId": "86b709f1-6e37-4522-b730-f15477868d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BaseCumulativeTransformOutputParser', 'BaseGenerationOutputParser', 'BaseLLMOutputParser', 'BaseOutputParser', 'BaseTransformOutputParser', 'CommaSeparatedListOutputParser', 'JsonOutputKeyToolsParser', 'JsonOutputParser', 'JsonOutputToolsParser', 'ListOutputParser', 'MarkdownListOutputParser', 'NumberedListOutputParser', 'PydanticOutputParser', 'PydanticToolsParser', 'SimpleJsonOutputParser', 'StrOutputParser', 'XMLOutputParser', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'base', 'format_instructions', 'json', 'list', 'openai_tools', 'pydantic', 'string', 'transform', 'xml']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnUM5Gt4kKV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0dVkaBjiYz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}