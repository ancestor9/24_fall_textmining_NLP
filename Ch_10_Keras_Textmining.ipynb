{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj9QxDl8UVERpFxD2pL1WX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/24_fall_textmining_NLP/blob/main/Ch_10_Keras_Textmining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras 입문\n",
        "\n"
      ],
      "metadata": {
        "id": "UPdX3LDY-xDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 전처리(Preprocessing)\n",
        "- **Tokenizer()**: 토큰화와 정수 인코딩을 위해 사용하며 훈련 데이터로부터 단어 집합을 생성하고, 해당 단어 집합으로 임의의 문장을 정수 인코딩하는 과정"
      ],
      "metadata": {
        "id": "odc6Wdsg_bCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "16SfSFacBmoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "X2CJq_Gs-9dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_train = [\n",
        "    'The earth is an great place live',\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "# Tokenization\n",
        "tokenizer.fit_on_texts(corpus_train)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"단어 집합 : \", tokenizer.word_index)"
      ],
      "metadata": {
        "id": "xfDzYo7Q_Jtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3849eae9-8c2f-4098-9d4e-b3b9655046c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 :  {'the': 1, 'is': 2, 'this': 3, 'document': 4, 'first': 5, 'earth': 6, 'an': 7, 'great': 8, 'place': 9, 'live': 10, 'second': 11, 'and': 12, 'third': 13, 'one': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_text = \"The earth is an awesome place live\"\n",
        "\n",
        "# 단어 집합 생성\n",
        "tokenizer.fit_on_texts([train_text])\n",
        "print(tokenizer)\n",
        "# 정수 인코딩\n",
        "sub_text = \"The earth is an great place live\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8GMZ-M7_y2S",
        "outputId": "ab1f3d55-b9af-4370-942a-ef740703c138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.src.legacy.preprocessing.text.Tokenizer object at 0x7bdf8edb1420>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# ## Keras 입문\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "train_text = \"The earth is an awesome place live\"\n",
        "# train_text = \"I am your father\"\n",
        "\n",
        "#단어 집합 생성\n",
        "tokenizer.fit_on_texts([train_text])\n",
        "\n",
        "# 결과 출력\n",
        "print(\"단어 집합 : \", tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmGkptWf_AqF",
        "outputId": "ba75cdd0-1a1c-403e-9af0-aa9c723a4cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩\n",
        "sub_text = \"The earth is an great place live\"\n",
        "sequences = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "\n",
        "print(\"정수 인코딩 : \",sequences)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"단어 집합 : \", word_index)\n",
        "\n",
        "# 행렬 생성\n",
        "matrix = np.zeros((len(sequences), len(word_index)))\n",
        "for i, seq in enumerate(sequences):\n",
        "  matrix[i, seq-1] = 1\n",
        "matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o72Mq8BX_GHi",
        "outputId": "7e88420d-c702-475a-9e75-0b5ad58be40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 :  [1, 2, 3, 4, 6, 7]\n",
            "단어 집합 :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pad_sequence() :\n",
        "- 전체 훈련 데이터에서 각 샘플의 길이는 서로 다를 수 있거나 각 문서 또는 각 문장은 단어의 수가 제 각각\n",
        "- 모델의 입력으로 사용하려면 모든 샘플의 길이를 동일하게 맞추어야 하는데 이를 자연어 처리에서는 패딩(padding) 작업\n",
        "- 보통 숫자 0을 넣어서 길이가 다른 샘플들의 길이를 맞추며 케라스에서는 pad_sequence()를 사용\n",
        "- pad_sequence()는 정해준 길이보다 길이가 긴 샘플은 값을 일부 자르고, 정해준 길이보다 길이가 짧은 샘플은 값을 0으로 채움"
      ],
      "metadata": {
        "id": "KdsCGtbAAERT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2oPWqpg_PwM",
        "outputId": "ad9e6cff-82d0-4411-c578-0096737fde5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [0, 7, 8]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtOXJCDVAcD3",
        "outputId": "bcb4b721-3baf-4e5d-8fcb-4990a38cb083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [7, 8, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 워드 임베딩(Word Embedding)\n",
        "- 워드 임베딩이란 텍스트 내의 단어들을 밀집 벡터(dense vector)로 만드는 것\n",
        "### **2.1 데이터전처리(토큰화)**"
      ],
      "metadata": {
        "id": "6REv3jtKAZxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 1. 토큰화된 텍스트\n",
        "tokenized_text = [['Hope', 'to', 'see', 'you', 'soon'], ['Nice', 'to', 'see', 'you', 'again']]\n",
        "\n",
        "# 2. Tokenizer 객체 생성\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# 3. fit_on_texts() 메서드로 단어 집합 생성\n",
        "# fit_on_texts()는 단어 집합을 생성할 때 토큰화된 리스트 형태를 받는 대신, 전체 텍스트를 받아야 함\n",
        "# 따라서 각 리스트를 문장으로 합친 형태로 전달\n",
        "tokenizer.fit_on_texts([' '.join(sentence) for sentence in tokenized_text])\n",
        "\n",
        "# 4. 각 텍스트를 정수 인코딩으로 변환\n",
        "encoded_text = tokenizer.texts_to_sequences([' '.join(sentence) for sentence in tokenized_text])\n",
        "\n",
        "# 5. 결과 출력\n",
        "print(\"단어 집합 : \", tokenizer.word_index)\n",
        "print(\"정수 인코딩 : \", encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYSNB734CIlw",
        "outputId": "e61029bb-3a7d-48b9-d23e-813a673f853d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 :  {'to': 1, 'see': 2, 'you': 3, 'hope': 4, 'soon': 5, 'nice': 6, 'again': 7}\n",
            "정수 인코딩 :  [[4, 1, 2, 3, 5], [6, 1, 2, 3, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2. WordEmbedding**"
      ],
      "metadata": {
        "id": "-uMhqH_RChVK"
      }
    },
    {
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "import numpy as np\n",
        "\n",
        "# 1. 토큰화된 텍스트\n",
        "tokenized_text = [['Hope', 'to', 'see', 'you', 'soon'], ['Nice', 'to', 'see', 'you', 'again']]\n",
        "\n",
        "# 2. Tokenizer 객체 생성\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# 3. 단어 집합 생성\n",
        "tokenizer.fit_on_texts([' '.join(sentence) for sentence in tokenized_text])\n",
        "\n",
        "# 4. 각 텍스트를 정수 인코딩으로 변환\n",
        "encoded_text = tokenizer.texts_to_sequences([' '.join(sentence) for sentence in tokenized_text])\n",
        "\n",
        "# 5. 패딩 적용 (시퀀스의 길이를 동일하게 맞추기 위해)\n",
        "padded_text = pad_sequences(encoded_text, padding='post', maxlen=5)\n",
        "\n",
        "print(\"단어 집합 : \", tokenizer.word_index)\n",
        "print(\"패딩된 정수 인코딩 :\\n\", padded_text)\n",
        "\n",
        "# 6. 임베딩 레이어 정의\n",
        "vocab_size = len(tokenizer.word_index) + 1  # 단어 집합의 크기 + 1 (0번 인덱스를 위한 추가)\n",
        "embedding_dim = 3  # 임베딩 차원 수\n",
        "input_length = padded_text.shape[1]  # 입력 시퀀스 길이 (패딩된 길이와 동일)\n",
        "\n",
        "# 7. 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
        "model.add(Flatten())  # 임베딩 벡터를 펼치기 위해 사용\n",
        "model.add(Dense(1, activation='sigmoid'))  # 예시로 간단한 이진 분류용 출력층 추가\n",
        "\n",
        "# 8. 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 9. 모델 요약 출력\n",
        "model.summary()\n",
        "\n",
        "# --- The fix ---\n",
        "# Create some dummy data for training\n",
        "X_train = padded_text\n",
        "y_train = np.array([0, 1])  # Example labels (adjust as needed)\n",
        "\n",
        "# Train the model for at least one epoch\n",
        "model.fit(X_train, y_train, epochs=1)\n",
        "# --- End of fix ---\n",
        "\n",
        "# 10. 가중치 추출 (임베딩 벡터)\n",
        "embedding_layer = model.layers[0]\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "# 11. 임베딩 벡터 출력\n",
        "print(\"임베딩 벡터 :\\n\", embedding_weights)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "gkR5EMEwDVvA",
        "outputId": "0ba6ef49-6b90-45ed-e200-cf41fa1c0e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 :  {'to': 1, 'see': 2, 'you': 3, 'hope': 4, 'soon': 5, 'nice': 6, 'again': 7}\n",
            "패딩된 정수 인코딩 :\n",
            " [[4 1 2 3 5]\n",
            " [6 1 2 3 7]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819ms/step - accuracy: 0.0000e+00 - loss: 0.7071\n",
            "임베딩 벡터 :\n",
            " [[-0.00655003  0.03789905 -0.01900787]\n",
            " [-0.04005739  0.02494198  0.04938038]\n",
            " [-0.03571683 -0.02770532 -0.04837917]\n",
            " [ 0.04056328  0.02016231 -0.04066862]\n",
            " [ 0.03829871 -0.03119194  0.02735214]\n",
            " [ 0.01206921 -0.02228847 -0.03972652]\n",
            " [-0.04865739  0.00128269  0.02605205]\n",
            " [-0.00189136 -0.01079234 -0.01497699]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color='orange'> 이상한 나라의 앨리스로 실습**"
      ],
      "metadata": {
        "id": "k2-QxSqEEnux"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "import numpy as np\n",
        "\n",
        "# 1. NLTK의 말뭉치에서 Alice in Wonderland 텍스트 불러오기\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# 2. 텍스트 전처리 (토큰화)\n",
        "# 단어 단위로 토큰화 수행\n",
        "tokens = nltk.word_tokenize(alice_text)\n",
        "\n",
        "# 3. 소문자 변환 및 알파벳 이외의 문자 제거\n",
        "tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# 4. TensorFlow Tokenizer 객체 생성 및 단어 집합 학습\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokens)\n",
        "\n",
        "# 5. 텍스트를 정수 인코딩으로 변환\n",
        "sequences = tokenizer.texts_to_sequences([tokens])[0]  # 전체 텍스트를 시퀀스로 변환\n",
        "\n",
        "# 6. 시퀀스의 길이를 동일하게 맞추기 위해 패딩 적용 (여기서는 최대 길이를 100으로 설정)\n",
        "max_length = 100\n",
        "padded_sequences = pad_sequences([sequences], maxlen=max_length, padding='post')\n",
        "\n",
        "print(\"단어 집합 크기: \", len(tokenizer.word_index))\n",
        "print(\"패딩된 시퀀스 샘플:\\n\", padded_sequences)\n",
        "\n",
        "# 7. 임베딩 레이어 정의 및 모델 구성\n",
        "vocab_size = len(tokenizer.word_index) + 1  # 단어 집합의 크기 + 1 (0번 인덱스를 위한 추가)\n",
        "embedding_dim = 50  # 임베딩 벡터의 차원 수\n",
        "input_length = max_length  # 입력 시퀀스의 길이\n",
        "\n",
        "# 8. 모델 정의\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
        "model.add(Flatten())  # 임베딩 벡터를 펼치기 위해 사용\n",
        "model.add(Dense(1, activation='sigmoid'))  # 예시로 간단한 이진 분류용 출력층 추가\n",
        "\n",
        "# 9. 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 10. 모델 요약 출력\n",
        "model.summary()\n",
        "\n",
        "# --- The fix ---\n",
        "# Create some dummy data for training\n",
        "# Assuming padded_sequences is your input data\n",
        "X_train = padded_sequences\n",
        "# Create dummy labels (replace with your actual labels)\n",
        "y_train = np.array([0])  # Example label, adjust as needed\n",
        "\n",
        "# Train the model for at least one epoch\n",
        "model.fit(X_train, y_train, epochs=1)\n",
        "# --- End of fix ---\n",
        "\n",
        "# 11. 임베딩 벡터 출력\n",
        "embedding_layer = model.layers[0]\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "print(\"임베딩 벡터 샘플 :\\n\", embedding_weights[:5])  # 임베딩 벡터 일부 출력"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "fvmdmwAxF-3m",
        "outputId": "70e6d0c4-0c58-4387-fce2-71fca3458aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 크기:  2470\n",
            "패딩된 시퀀스 샘플:\n",
            " [[  75 1348  774 2460    5 2461    3   51   80   29  173   30  362    8\n",
            "   772   41   12    1   22   51    4  476 1158    2   80    5   41  355\n",
            "   245   21   16 2462 2463    1  569    2 2464 1346    8   16 2465    2\n",
            "    80    5   41 2466   40   16  111   30  342    2  154   83  147  455\n",
            "     2  979   20  289    4  585  699  246  194   20    1  449    8  786\n",
            "     8  130 1089    2   80    5   41  394   20   21   83  569 2467    2\n",
            "   184    4 1009   12   21   83  569 2468 2469   16  329    2    1 2470\n",
            "  1378  750]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850ms/step - accuracy: 1.0000 - loss: 0.6799\n",
            "임베딩 벡터 샘플 :\n",
            " [[-3.90512459e-02  7.28512928e-03 -1.45345815e-02 -1.96779966e-02\n",
            "   8.02484900e-03 -3.21628340e-02 -4.37870622e-05 -2.70767100e-02\n",
            "  -4.34693471e-02  8.99610668e-03 -4.22043465e-02 -4.09283489e-03\n",
            "  -1.49199367e-02 -3.35379839e-02  2.43359320e-02  3.54259945e-02\n",
            "   4.00505327e-02 -1.98069215e-02 -4.05947194e-02 -8.00559670e-03\n",
            "   1.82906128e-02  7.87124783e-03  4.09009717e-02  2.28908099e-02\n",
            "   1.18293278e-02 -2.51850616e-02 -3.59941609e-02 -2.20951792e-02\n",
            "   3.97859924e-02 -3.81585956e-02 -1.65395960e-02  6.03485107e-03\n",
            "   8.70371982e-03  1.92116760e-02 -1.47921927e-02  1.96578018e-02\n",
            "   9.15575027e-03 -6.73825666e-03 -1.56424418e-02  4.01898064e-02\n",
            "   1.48371346e-02 -3.07479985e-02 -6.58057630e-04 -1.55031197e-02\n",
            "  -1.69162378e-02  2.19146349e-02 -4.05705795e-02  1.20314211e-03\n",
            "   3.32486629e-03  1.06726401e-02]\n",
            " [ 1.51187275e-02  4.74952199e-02  1.25219952e-03  1.34195369e-02\n",
            "   2.70234831e-02 -4.79270816e-02  3.96604910e-02  1.75795592e-02\n",
            "  -3.32901813e-02  2.35374961e-02  4.41035144e-02  2.52002757e-02\n",
            "   2.17516646e-02 -4.80361357e-02  1.59052212e-03  4.94206436e-02\n",
            "  -5.18697361e-03 -2.63746269e-03 -2.38752849e-02 -1.72957815e-02\n",
            "  -1.88854430e-02  4.89585400e-02 -2.47840788e-02  3.26695815e-02\n",
            "   4.65795733e-02 -8.70815420e-04 -3.79518196e-02  1.12474551e-02\n",
            "  -1.83268562e-02  4.30272669e-02 -4.93334122e-02 -4.03410569e-02\n",
            "  -2.69845817e-02 -2.97934376e-02  1.53670902e-03 -8.01369175e-03\n",
            "  -4.10114303e-02 -3.19636986e-02 -3.77117805e-02  4.26995493e-02\n",
            "   9.86745115e-03 -4.17729095e-02 -2.25061588e-02 -2.92016286e-02\n",
            "  -2.80552804e-02 -2.78925300e-02 -1.88166499e-02  4.01737951e-02\n",
            "  -4.01999317e-02 -1.25095593e-02]\n",
            " [-4.84797508e-02  3.19320224e-02  1.17210578e-02 -5.49797527e-03\n",
            "   1.08300308e-02 -4.36923541e-02 -1.95484445e-03  2.38176249e-02\n",
            "   1.14444988e-02  3.24841514e-02 -4.05060723e-02 -2.54237615e-02\n",
            "  -4.42643724e-02  5.00543565e-02 -3.37156355e-02 -2.88703144e-02\n",
            "   3.80264372e-02 -1.20241204e-02 -4.18085456e-02  2.21632849e-02\n",
            "   2.20781900e-02 -1.81067921e-02  4.87570427e-02 -1.82126593e-02\n",
            "  -3.45954895e-02  1.26009015e-02 -1.81451049e-02  1.89422537e-02\n",
            "  -8.93675443e-03  2.12431122e-02  3.92498709e-02 -7.83152878e-03\n",
            "   3.60727347e-02  3.26595604e-02  3.06413434e-02  3.36941443e-02\n",
            "   4.28916104e-02 -4.38860767e-02 -4.31724191e-02 -3.16952402e-03\n",
            "   3.28580625e-02 -1.74777899e-02  4.80692508e-03  2.35052742e-02\n",
            "  -3.50477248e-02 -3.48155797e-02 -3.23870853e-02  2.50048507e-02\n",
            "   1.19864624e-02  3.51258218e-02]\n",
            " [-1.63266268e-02  4.96570300e-03 -2.56565530e-02 -2.68380288e-02\n",
            "   3.04133911e-02  8.40209704e-03  4.81409542e-02 -2.31085718e-02\n",
            "   2.85031851e-02  5.78232203e-03 -5.66623453e-03 -4.08794060e-02\n",
            "   3.74312922e-02  1.93626173e-02 -2.55248640e-02 -1.23240193e-02\n",
            "   4.94371578e-02 -2.16064695e-02 -3.60876136e-02  3.07930745e-02\n",
            "  -4.73826341e-02  2.05314774e-02 -2.12536249e-02 -2.65813172e-02\n",
            "  -4.38833013e-02  2.23351531e-02  4.89234328e-02  3.62121239e-02\n",
            "   4.49168868e-02 -4.12307531e-02 -2.74104923e-02  4.39528227e-02\n",
            "  -3.09465034e-03 -3.03328745e-02  2.42462493e-02 -2.73927413e-02\n",
            "  -2.11054366e-02 -4.28136885e-02  4.25488651e-02 -9.99326538e-03\n",
            "   7.18861865e-03 -1.64279044e-02  8.28867592e-03  1.85049127e-03\n",
            "  -2.70735025e-02 -1.30893113e-02  1.25981765e-02 -9.00832005e-04\n",
            "   2.78315227e-02  4.70977426e-02]\n",
            " [ 1.25263091e-02  1.54183889e-02 -1.53591176e-02 -3.66724655e-02\n",
            "  -4.89462912e-02  2.38501634e-02  3.28116938e-02 -1.49203558e-02\n",
            "   4.48837802e-02 -1.33359537e-03  1.01525802e-03 -8.48551933e-03\n",
            "  -2.67264992e-03 -2.84191966e-02 -4.42244634e-02  2.11183485e-02\n",
            "   3.50916721e-02  4.35193554e-02  2.28799358e-02  6.25156146e-03\n",
            "  -2.72101425e-02  4.59876508e-02  2.10266896e-02 -1.69903636e-02\n",
            "   2.72182263e-02  1.08150998e-02 -1.03545897e-02  1.28845219e-02\n",
            "  -3.85880135e-02  2.96559706e-02 -3.36379930e-02 -5.69645641e-03\n",
            "  -1.63714252e-02  1.06679602e-02  1.94675997e-02  2.69399956e-02\n",
            "  -3.03051118e-02  1.12746228e-02  2.31681745e-02 -4.01279740e-02\n",
            "   1.97851658e-02  3.84955443e-02 -8.51107109e-03 -3.15094192e-04\n",
            "   3.82600017e-02 -2.14654463e-03  1.62595790e-02 -3.14752236e-02\n",
            "   5.52017475e-03  1.80601683e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[projector.tensorflow.org](https://projector.tensorflow.org/)**\n",
        "- 시각화를 위한 데이터 추출"
      ],
      "metadata": {
        "id": "7rW-OmDRJC40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 4. 벡터 및 메타데이터 파일 생성\n",
        "# 디렉토리 생성\n",
        "output_dir = \"/content\"# 12. 벡터 및 메타데이터 파일 생성\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# vectors.tsv 생성\n",
        "vectors_file = os.path.join(output_dir, 'vectors.tsv')\n",
        "with open(vectors_file, 'w', encoding='utf-8') as f:\n",
        "    for vector in embedding_weights:\n",
        "        f.write('\\t'.join(map(str, vector)) + \"\\n\")\n",
        "\n",
        "# metadata.tsv 생성\n",
        "metadata_file = os.path.join(output_dir, 'metadata.tsv')\n",
        "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"Word\\n\")  # 첫 번째 행에 헤더 추가\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        f.write(word + \"\\n\")\n",
        "\n",
        "print(\"임베딩 벡터와 메타데이터 파일이 생성되었습니다.\")\n",
        "print(f\"벡터 파일 경로: {vectors_file}\")\n",
        "print(f\"메타데이터 파일 경로: {metadata_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrQZ-pbaFpWG",
        "outputId": "6d18c65a-bb2b-44f8-f65f-4828751b3c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 벡터와 메타데이터 파일이 생성되었습니다.\n",
            "벡터 파일 경로: /content/vectors.tsv\n",
            "메타데이터 파일 경로: /content/metadata.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--bhUFwfHZQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}